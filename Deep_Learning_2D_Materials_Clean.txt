2000953 ()  www.advmat.deCommuniCation Deep-Learning-Enabled Fast Optical Identification and Characterization of 2D Materials Bingnan Han, Yuxuan Lin,* Yafang Yang, Nannan Mao, Wenyue Li, Haozhe Wang, Kenji Yasuda, Xirui Wang, Valla Fatemi, Lin Zhou, Joel I.-Jan Wang, Qiong Ma, Yuan Cao, Daniel Rodan-Legrain, Ya-Qing Bie, Efrén Navarro-Moratalla, Dahlia Klein, David MacNeill, Sanfeng Wu, Hikari Kitadai, Xi Ling, Pablo Jarillo-Herrero,* Jing Kong,* Jihao Yin,* and Tomás Palacios* B. Han, W. Li, Prof. J. Yin Image Processing CenterSchool of AstronauticsBeihang UniversityBeijing 100191, ChinaE-mail: jihaoyin@buaa.edu.cn B. Han, Dr. Y . Lin, Dr. N. Mao, H. Wang, Dr. L. Zhou, Prof. J. Kong, Prof. T. PalaciosDepartment of Electrical Engineering and Computer ScienceMassachusetts Institute of TechnologyCambridge, MA 02139, USAE-mail: liny@mit.edu; jingkong@mit.edu; tpalacios@mit.edu Dr. Y . Yang, Dr. K. Yasuda, X. Wang, Dr. V. Fatemi, Dr. Q. Ma, Y . Cao, D. Rodan-Legrain, Dr. Y .-Q. Bie, D. Klein, Dr. D. MacNeill, Dr. S. Wu, Prof. P. Jarillo-HerreroDepartment of PhysicsMassachusetts Institute of TechnologyCambridge, MA 02139, USAE-mail: pjarillo@mit.eduDr. J. I.-J. Wang, Prof. J. KongResearch Laboratory of ElectronicsMassachusetts Institute of TechnologyCambridge, MA 02139, USA Prof. E. Navarro-Moratalla Instituto de Ciencia MolecularUniversidad de Valenciac/Catedrático José Beltrán 2, Paterna 46980, Spain H. Kitadai, Prof. X. Ling Department of Chemistry and Division of Materials Science and EngineeringBoston UniversityBoston, MA 02215, USA DOI: 10.1002/adma.202000953Research on 2D materials has grown exponentially over the past decade.[1,2] Hundreds of 2D materials have been isolated and studied,[2,3] offering a wide range of optical and electronics proper ties, including metals, semiconductors, insulators, magnets, and superconductors. The most widely used approach to obtain high-quality 2D crystals today is mechanical exfoliation, [4] followed by 2D crystal “hunting” under an optical micro-scope (Figure 1, left). This task is timeconsuming and difficult especially for inexperienced researchers, but tedious for experienced researchers. There has, therefore, been growing interest in automating such a process. [5] An automatic optical identification and/or characterization system requires an algorithm that performs reliably for different materials with high accuracy, is fast enough for real-time processing, and is easily adaptable to different optical setups and different user Advanced microscopy and/or spectroscopy tools play indispensable roles in nanoscience and nanotechnology research, as they provide rich informa-tion about material processes and properties. However, the interpretation of imaging data heavily relies on the “intuition” of experienced researchers. As a result, many of the deep graphical features obtained through these tools are often unused because of difficulties in processing the data and finding the correlations. Such challenges can be well addressed by deep learning. In this work, the optical characterization of 2D materials is used as a case study, and a neural-network-based algorithm is demonstrated for the material and thickness identification of 2D materials with high prediction accuracy and real-time pro-cessing capability. Further analysis shows that the trained network can extract deep graphical features such as contrast, color, edges, shapes, flake sizes, and their distributions, based on which an ensemble approach is developed to predict the most relevant physical properties of 2D materials. Finally, a transfer learning technique is applied to adapt the pretrained network to other optical identification applications. This artificial-intelligence-based material charac-terization approach is a powerful tool that would speed up the preparation, initial characterization of 2D materials and other nanomaterials, and potentially accelerate new material discoveries. The ORCID identification number(s) for the author(s) of this article can be found under https://doi.org/10.1002/adma.202000953. , 32, 2000953

2000953 () www.advmat.de www.advancedsciencenews.comrequirements with minimal additional human efforts. However, existing identification approaches[6] only exploited the information about the optical contrast (Figure S1, Supporting Informa-tion) of the 2D crystals. As a result, they are often limited to specific types of 2D crystals and microscope conditions, which do not meet the aforementioned requirements. In reality, optical microscopy (OM) images contain rich, often unused information other than optical contrast. These deep graph-ical features can be extracted through deep learning, especially semantic segmentation methods based on convolutional neural networks. [7,8] In this work, an encoder–decoder semantic segmentation network[8] is configured for pixel-wise identification of OM images of 2D materials. We call our method 2D material optical identification neural network (2DMOINet). We demonstrate that this architecture can identify, in real time, various 2D materials in OM images regardless of variations in optical setups. This would free up tremendous amount of time for researchers working on 2D materials. Additionally, we find that the algorithm finds corre-lations between the OM images and physical properties of the 2D materials and can thereby be used to anticipate the properties of new, as-yet uncharacterized 2D crystals. Figure 1 illustrates the flow chart of our 2DMOINet method. Figure 1a,c shows 13 different 2D materials used for training, their crystal structures, photos of their bulk (3D) source crys-tals, [9] and representative OM images of exfoliated 2D crystallites (or “flakes”) on top of 285 or 90 nm SiO 2/Si substrates. These materials are graphene/graphite, hexagonal boron nitride (hBN), the 2H-phase semiconducting transition-metal dichalcogenides (TMDs) 2H-MoS 2, 2H-WS 2, 2H-WSe 2, and 2H-MoTe 2, the 2H-phase metallic TMDs 2H-TaS 2 and 2H-NbSe 2, the 1T-phase TMD 1T-HfSe 2, black phosphorous (BP), the metal trihalides CrI 3 and RuCl 3, and the quasi-1D crystal ZrTe 5. A total number of 817 OM images containing exfoliated flakes of these 13 materials and 100 background-only images were collected , 32, 2000953 Figure 1. The flow chart of the proposed deep learning based optical identification method. We select 13 typical 2D materials; a) crystal structure and photographs[9] of the bulk crystals are shown. Photographs for CrI 3, RuCl 3, and ZrTe 5: Reproduced with permission.[9a] Copyright, 2D Semiconductors. Photographs for graphite, hBN, 2H-MoS 2, 2H-WS 2, 2H-WSe 2, 2H-MoTe 2, 2H-TaS 2, 2H-NbSe 2, 1T-HfSe 2, and black phosphorus: Reproduced with permission.[9b] Copyright, HQ Graphene. After mechanical exfoliation, the 2D flakes are randomly distributed on SiO 2/Si substrates. We then use different optical microscopes to take the images (b). c) Representative optical microscopy (OM) images of the 13 materials. d) When inputting these images to the trained 2DMOINet, e) the label maps will be predicted that segment individual 2D flakes and provide the labels (materials identities and thicknesses) of them. The 2DMOINet is composed of a series of convolutional layers, batch normalization layers, rectified linear unit (ReLU) layers (in blue), pooling (down-sampling) layers (in green), up-sampling layers (in orange), as well as a soft-max layer (in yellow) as the output layer. We define depth as the layers in between two adjacent down-sampling or up-sampling layers, denoted as D1 to D5. Note that the OM images displayed here and in Figures 3 and 5 are the original high-resolution images. The images for the input of the 2DMOINet were down-sampled to 224 × 224 pixels, and can be found in Figures S3,S14–S35, Supporting Information. Scale bars in (c): 20 µm. 15214095, 2020, 29, Downloaded from https://advanced.onlinelibrary.wiley.com/doi/10.1002/adma.202000953 by Pennsylvania State University, Wiley Online Library on [19/07/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

2000953 () www.advmat.de www.advancedsciencenews.comand labeled for training and testing. To make the training data representative of the typical variability of OM images, the training and test data were sampled from a collection of OM images generated by at least 30 users from 8 research groups with 6 different optical microscopes over a span of 10 years. The data were labeled with the material identifications and thick-nesses in a pixel-by-pixel fashion (ground-truth label maps) with the help of a semi-automatic image segmentation program (Figure S2, Supporting Information). After color normalization and image resizing (Figure S1, Supporting Information), a training dataset of 3825 RGB images and a test dataset of 2550 images, both with the size of 224 × 224 pixels, were generated from these 917 OM images. During training, a random rotation data augmentation method which produce random positioning and orientation of images were utilized. Detailed information is shown in Supporting Information. A stochastic gradient descent with momentum method [10] was used to train the weights in the convolutional filters (local and linear operators applied to the images to extract simple graphical features) of the 2DMOINet. The 2DMOINet (the schematic is shown in Figure 1d) consists of a series of down-sampling layers (encoder) and a corresponding set of up-sampling layers (decoder) followed by a pixel-wise classifier. Each down- (up-) sampling layer reduce (increase) the size of the image from the previous layer. We refer to the layers in between two down- or up-sampling layers as layers in different depth (depth = 1–5, or shortened to D1 to D5, as shown in Figure 1d). We selected a well-known network structure called VGG16 [11] as the encoder network in the 2DMOINet, and the detailed information about the network structure, the data generation and augmentation, as well as the network training process can be found in Figure 1 and Figure S1, Table S1, Supporting Information. We first show that the trained 2DMOINet can be used to segment the OM images among 13 different exfoliated 2D materials and find the material identity and thickness of each flake. The performance can be visualized in Figure 1, Table 1, and Figure S3, Supporting Information. The color maps in Figure 1e are the typical label maps predicted by the trained 2DMOINet with the corresponding OM images in Figure 1c as the input to the 2DMOINet. Figure S3, Supporting Information, shows additional results of the test OM images, the ground-truth label maps (labeled semi-automatically by humans according to the materials and thicknesses), as well as the predicted label maps. In addition to material separability, we labeled four materials (graphene/graphite, 2H-MoS 2, 2H-WS 2, and 2H-TaS 2) with dif ferent thicknesses (1L for monolayer, 2–6L for bilayer to 6-layer, and >6 for greater than 6 layers) to verify the thickness differ entiation capability, which is a particularly important task. The trained 2DMOINet is able to outline individual flakes from the background and distinguish both the material identities and thicknesses of the thirteen 2D materials with high success rate. After a systematic optimization in terms of the network structure, the data augmentation approach, and the addition of background-only images (see Supporting Information for detailed information), we eventually obtained a trained 2DMOINet with high prediction accuracy, fast processing speed, as well as good generality that can be used for a fully automatic 2D mate-rial searching system. In the following, we discuss these three important aspects of our deep-learning-based optical identification approach in more details. In the Supporting Information, we select three baseline methods (two image processing methods and one simple machine learning method) and com-pare their performance with our 2DMOINet method. We first analyzed the prediction accuracy of the 2DMOINet. Figure 2 presents the pixel-level (Figure 2a–e) and the flake-level (Figure 2f–j) confusion matrices of the test dataset (see Sup-porting Information for more details). The diagonal elements are the success rate of each class, and the off-diagonal elements are the rate of misclassified pixels or flakes in the test OM images. The classification accuracies of both material identities (Figure 2a,f) and thicknesses (Figure 2b–e,g–j) are mostly above 70% and the mean class accuracy is 79.78%. Additional perfor mance metrics are summarized in Table 1. The overall accuracy counted by pixels reaches 96.89%; and the mean intersection over union (IoU, defined as the intersection of the ground truth and the predicted region of a specific label over the union of them) is 58.78% for the training dataset. These values are much better than baseline methods (Table S5, Supporting Information). As will be discussed later, the accuracy of our network shown here is very promising especially for applications of automating the initial scanning and screening of exfoliated 2D materials. Note that the calculated performance metrics of the 2DMOINet are likely an underestimate: after careful inspection of the label maps predicted by the 2DMOINet, we discovered a number of OM images in which the ground-truth was initially mislabeled, but predicted correctly by the 2DMOINet (Figure S4, Supporting Information), which contributes tremendously to the thickness identification errors. This scenario is consid-ered as a classification mistake in the above metrics. On the other hand, it is observed that many of the mistakes made by the network are due to the similarities between different materials. For example, misclassification rates among 2H-MoS 2, 2H-WS 2, 2H-WSe 2, and 2H-MoTe 2 are as high as 12%, which is probably a consequence of their similar crystal structures and optical properties. Although this type of misclassification is inevitable and detrimental to the prediction accuracies for material identity, it contains the information about the similari-ties between different materials which could be harnessed to construct material property predictors as will be discussed later. The third type of common mistake is that metal markers, tape adhesive residue, and text labels in the OM images were misi-dentified as a 2D material (Figure S4, Supporting Information). These non-2D material features were labeled as “background” together with the blank substrate in the ground-truth, but they have special structures and high color contrast relative to the substrate, thereby confusing the network. In a future version of the network, this may be solved by either further increasing , 32, 2000953Table 1. Overall classification performance of the 2DMOINet. The global accuracy (by pixel), the mean accuracy (by class), and the mean IoU (by pixel) are all calculated on the test dataset. The experiment environment is: CPU: Intel(R) Core(TM) i7-8700K CPU @ 3.70 GHz, 32.0GB RAM; GPU: NVIDIA GeForce GTX 1080 Ti, 11 GB GDDR5X. Global accuracy (by pixel)Mean accuracy (by class)Mean IoU (by pixel)Training time Frames per second (FPS) in test Use CPU Use GPU 0.9689 0.7978 0.5878 30 h 56 min 23 s 2.5 22.0 15214095, 2020, 29, Downloaded from https://advanced.onlinelibrary.wiley.com/doi/10.1002/adma.202000953 by Pennsylvania State University, Wiley Online Library on [19/07/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

2000953 () www.advmat.de www.advancedsciencenews.comthe training dataset or introducing dedicated labels for these non-2D-material features. The former potential solution has been partially confirmed by our results. We observed a drop of the misclassification rate for the non-2D-material features if we compare the training results with the random-rotation augmentation method and additional background-only images, and those with the basic data augmentation method (Figure S9, Supporting Information). The fourth common mistake is inaccuracy in the profiles of the flakes. This usually happens when the profiles are very complex, or if the flakes are highly fragmentary (Figure S4, Supporting Information). These mistakes are mainly due to the down-sampling of the encoder layers in the 2DMOINet, which inevitably drops the high-frequency spa-tial features of the images. We believe that the proposed deep learning algorithm is fast enough and well suited to real-time processing according to the metrics given in Table 1. With our computing environment (see Supporting Information), the training process for the VGG16 2DMOINet requires 30 h with a GPU, whereas the testing speed can be as high as 2.5 frames per second (fps) using a CPU, and 22 fps using a GPU for 224 × 224 pixel test images. This means the 2DMOINet, once properly trained, can be easily adapted to standard desktop computers and integrated with different optical microscopes with automatic scanning stages for fast or even real-time identification. As a proof-of-concept demonstration, we have integrated our 2D material identifica-tion algorithm to an optical microscope with a motorized XYZ stage. The motorized stage is able to scan across one sample (1.5 cm × 1.5 cm) and capture over 1000 high-resolution OM images (1636 × 1088 pixels) within 50 min. These images can then be pre-processed (color normalization, resize) and fed to the trained 2DMOINet for their corresponding label maps in less than 15 min with a GPU-equipped computer. Figures S10, S11, and Videos S1,S2, Supporting Information, show the thick-ness identification results for an exfoliated graphene sample and an exfoliated MoS2 sample. The frames outlined in red in Figures S10 and S11, Supporting Information, are the identified monolayer and fewlayer samples. 9.45% and 8.25% of the total images are selected for the graphene and MoS2 examples, respectively. These selected images can be further screened by human researchers within 5 min. Relocation of the appropriate flakes can then be implemented automatically for following , 32, 2000953 Figure 2. Confusion matrices calculated from the test results. a–e) Pixel-level confusion matrices, and f–j) flake-level confusion matrices (see Supporting Information). (a) and (f) are for material identities. (b–e) and (g–j) are for thicknesses. In each confusion matrix, the diagonal terms are the success rate of the predicted classes, and the non-diagonal terms are the rate of misclassified pixels. The mean class prediction accuracy is 79.8%. 15214095, 2020, 29, Downloaded from https://advanced.onlinelibrary.wiley.com/doi/10.1002/adma.202000953 by Pennsylvania State University, Wiley Online Library on [19/07/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

2000953 () www.advmat.de www.advancedsciencenews.comstudies. Additional information about this automatic 2D-material-searching system can be found in Supporting Information. Furthermore, through the transfer learning technique discussed later, it is possible to further improve the thickness identification accuracy for a single material. This thickness identification tool has high identification accuracy and high throughput, and has already been utilized in daily laboratory research. We would like to emphasize that our deep-learning-based optical identification approach is generic for identification of different 2D materials, and has good robustness against variations such as brightness, contrast, white balance, and non-uniformity of the light field (Figure S1, left, Supporting Information). For this, we intentionally included OM images taken under different optical microscopes and different users as mentioned previously. As a comparison, existing optical identi-fication methods are completely based on optical contrast of the 2D crystals, [6] and as a result, they are often specific to types of 2D crystals, conditions and configurations of the microscopes being used, image qualities, and so on. In addition, optical contrast based methods would fail for harder problems in which the classes to be differentiated are not separable in the color space, such as identifying the materials in unlabeled optical images (Figure S12, Supporting Information). Our results suggest that after training, the same 2DMOINet is able to identify both the material identities and their thicknesses among 13 different 2D materials with high accuracy and fast speed from OM images regardless of different conditions of the microscopes, which is fundamentally different and more practical as compared to optical-contrast-based approaches (see Supporting Information for more discussions). Another important feature we discovered from our experiment is that the trained 2DMOINet was even able to identify the OM images that are out of focus (Figure S5, Supporting Information). This further proves the robustness of our algorithm. To interpret how the 2DMOINet extracts features from 2D material OM images, we analyzed the output feature maps of all the layers in the trained network for the OM images in the test dataset as the inputs. As a demonstration, we used a typical image of graphite/graphene (shown in Figure S13a, Supporting Information) as the input. The corresponding convolutional fea-ture maps of all the layers in the encoder, decoder, and output sections of the 2DMOINet are summarized in Figure S13b–d, Supporting Information. Taking the convolutional feature maps in the encoder as an example (Figure S13b, Supporting Informa-tion), we can clearly see that D1 feature maps are highly corre-lated to color and contrast information. In this shallow layer, the background and monolayer region of graphene are not easily separable because of the weak contrast between the two classes, whereas multilayer graphene region is already quite distinguish-able. In D2 feature maps, more boundary characteristics are detected, and the edges of graphene monolayer regions start to stand out in some of the feature maps. With the increase of the depth, the size of each feature map becomes smaller because of the pooling layers in the network, and the receptive field of each convolutional kernel (the area in the input image each neuron in the current layer can respond to) becomes relatively larger, which leads to a higher level abstraction of the global graph-ical features. For instance, Figure S13e, Supporting Informa-tion, displays the most prominent feature map (channel #186) of the D5 encoder layer with the largest activation value. It is observed that this feature map is highly correlated to the monolayer graphene region. By feeding the network with more test images as summarized in Figure S22, Supporting Information, we further confirmed that channel #186 is sensitive to pink/ purple flakes with large flake sizes, smooth edges, and regular shapes. After further analysis on the 512 channels of the D5 encoder layer with more test images randomly chosen from our database, we conclude that the trained 2DMOINet is able to capture deep and subtle graphical features that were overlooked by previously reported optical contrast based approaches. [6] Many of the deep graphical features reflect in part the physical properties of the 2D materials. To illustrate this, we select 21 easily inter preted channels and discuss their associated graphical features and the related physical properties as summarized in Figure 3, Figures S14–S34, and Tables S6,S7, Supporting Information. We divide the graphical features captured by these channels into four broad categories: 1) contrast or color, 2) edge or gradient, 3) shape, and 4) flake “size”. Figure 3 shows the heat maps of several channels that belong to each category. In particular, channels #186, #131, and #230 under the “contrast/color” cat-egory (Figure 3b) are sensitive to flakes with purple/pink, yellow/green/grey, and bright green/blue colors, respectively; channels #87 and #488 under the “edge” category (Figure 3c) reveal smooth all-direction edges and bottom edges, respec-tively; channels #52, #436, and #279 under the “shape” category (Figure 3e) are indicators of shapes with edges at acute angles, slender shapes, and 1D straight wires; and channels #230 and #484 under the “flake size” category capture large and small/fragmentary flakes. Table S6 and Figures S14–S34, Supporting Information, provide more OM images, their corresponding heat maps as well as the extracted graphical features of the 21 channels. Note that some channels can only respond to images that meet a combinational criterion under multiple categories, whereas some channels can be sensitive to several scenarios. For example, channel #87 only shows high intensities in the heat map around the smooth edges of purple flakes (Figure S16, Supporting Information), and channel #312 can be used to iden-tify both non-uniform, thick flakes, and uniform, thin, purple/pink 1D wires (Figure S27, Supporting Information). To confirm that the above analysis about the deep graphical feature maps are generic for our problem but not specific to any individual 2DMOINet, we repeated the training procedure for the 2DMOINet with different initialization, random permutation of the dataset, and different data augmentation strategies. 16 training runs were performed separately with the metrics and pixel-level confusion matrices shown in Table S4 and Figures S37–S52, Supporting Information. Similar behaviors are manifested in all these 2DMOINets. Moreover, we can easily identify similar functionalities among the D5 channels in these models. In Figure S35, Supporting Information, four pairs of channels that belong to two independently trained networks (network #1 and network #2 in Table S4, Supporting Information) are displayed side by side. Although the channel indices are different in each pair, they all have similar feature activation behavior on the same input OM images. In particular, channel #153 in network #1 and channel #227 in network #2 correspond to low-contrast purple/pink flakes with smooth edges; channel , 32, 2000953 15214095, 2020, 29, Downloaded from https://advanced.onlinelibrary.wiley.com/doi/10.1002/adma.202000953 by Pennsylvania State University, Wiley Online Library on [19/07/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

2000953 () www.advmat.de www.advancedsciencenews.com#13 in network #1 and channel #433 in network #2 correspond to left edges of low-contrast purple/pink flakes; channel #490 in network #1 and channel #433 in network #2 correspond to 1D wires; and channel #76 in network #1 and channel #490 in network #2 correspond to fragmentary/small flakes. Another interesting observation regarding the gener ality of the graphical feature found by our algorithm is that the 2DMOINet is able to learn additional features in the D5 channels in order to compensate the artifacts induced by the random rotation data augmentation strategy. In Figure S36b,c, Supporting Information, it is clearly observed that a circular region appears in the feature map using random rotation method, and no such pattern emerges in basic augmentation method. This difference is caused by a padding procedure used during image rotation in every epoch. However, we find that the network amends this “error” and predict a correct map as shown in Figure S36d, Supporting Information. Furthermore, it is clearly seen that this artifact has been mostly removed in the output of the graphene channels in the final rectified linear unit (ReLU) layer of the network using random rotation augmentation method as shown in Figure S36e, Supporting Infor mation. We also found that additional channels that are only sensitive to the perimeters of the squared images are generated by the models with random rotation augmentation (channels #156, #402, and #425 in Table S6, Supporting Information; see examples in the inset of Figure S36b and Figures S20,S29,S30, Supporting Information). These features are much less prominent in the model with the basic data augmentation strategy. Therefore, we believe that these channels related to image perimeters are responsible for the compensation of the cir cular artifacts induced by the random rotation augmentation. The above feature map analysis has provided a better under standing about how deep graphical features can be extracted by the 2DMOINet for more accurate and generic optical , 32, 2000953 Figure 3. Deep graphical features captured by the 2DMOINet. a) Schematics of the physical properties such as the band structure and the thickness that determine the optical responses of the 2D flakes. b) Contrast/color and c) edge and typical feature maps in the D5 layer of the 2DMOINet that are associated with the optical responses. d) Schematics of the physical properties such as the crystal symmetry, the mechanical anisotropy, and the exfoliation energy that determine the mechanical exfoliation of the 2D flakes. e) Flake shape and f) flake size and typical feature maps in the D5 layer of the 2DMOINet that are correlated to the mechanical properties of the materials. The high-activation regions in the feature maps are also indicated by red dashed curves in the corresponding OM images. Scale bars: 20 µm. 15214095, 2020, 29, Downloaded from https://advanced.onlinelibrary.wiley.com/doi/10.1002/adma.202000953 by Pennsylvania State University, Wiley Online Library on [19/07/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

2000953 () www.advmat.de www.advancedsciencenews.comidentification of exfoliated 2D materials. However, the algorithm is not limited to this particular task, and we found that it can be used for more advanced optical characterization tasks such as the prediction of material properties. For this, the graphical features captured by the network are correlated to the optical and mechanical properties of the material. As shown schematically in Figure 3a, the contrast/color and the edge features are determined by the optical response of the material, which reflect the electronic band structures and the thicknesses of the flakes. In addi-tion, because the samples were made through mechanical exfo-liation, the typical distributions of flake shapes and sizes heavily depends on the mechanical properties of the materials, such as the crystal symmetry, mechanical anisotropy, and the exfoliation energy (Figure 3d). We can thus use the network trained with the knowledge of the 13 materials and do further statistical analysis to predict properties of materials not present in the training set. One simple approach to implement the material property predictor is to take the “similarity” vectors of each material from the confusion matrix (each row of the matrix) and decom-pose the vectors to a set of specifically designed base vectors. To improve the fidelity of this simple and relatively weak predictor, an ensemble approach is employed. [12] We performed such decomposition operation on the 16 2DMOINets (Table S4, Supporting Information) that were trained independently, and took the average of these predictors as the final prediction. In this way, the projected values can give us a probabilistic predic-tion of the physical property of interest (more details are given in Supporting Information). As a proof-of-concept demonstration, we fed the trained network with OM images of 17 2D materials that were unknown to the network during the training stage. The extended confu-sion matrices containing the 13 trained materials and the 17 untrained materials is shown in Figures S37–S52, Supporting Information, and the physical properties of these 2D materials are summarized in Table S7, Supporting Information. As we can see, different vector components in the similarity vectors (or columns of the extended confusion matrix) have distinct values for each of the untrained materials, from which we can immediately summarize some qualitative patterns. For example, GaS, CrI 3, CrBr 3, and MnPS 3 in the untrained material group shows high similarity to hBN in the trained material group, which matches the fact that these materials are wide-bandgap semiconductors or insulators with the bandgaps higher than 2.5 eV and are mostly transparent in the infrared, red, and green spec-tral ranges. As another example, 1T’-MoTe 2 and Td-WTe 2 in the untrained material group are predicted to be similar to 1T-HfSe 2 , 32, 2000953Figure 4. Prediction of physical properties through an ensemble approach based on 16 independently trained 2DMOINets. a) Histograms of the projected values of the materials in the training set (top half) and in the prediction set (bottom half, unused when training the 2DMOINets) that predict the bandgap of the materials. b) Histograms of the projected values that predict the crystal structure of the materials. The materials are regrouped by the bandgaps and the crystal structures respectively in (a) and (b). The histograms and the error bars are the mean projected values and their standard deviations of the material property predictor constructed based on 16 independently trained 2DMOINet. The training set contains the 13 materials used for the training of the 2DMOINet, whereas the prediction set contains 17 additional materials that are unknown to the 2DMOINet during the training stage. The materials in the grey box in (b) do not belong to any of the crystal structure classes. 15214095, 2020, 29, Downloaded from https://advanced.onlinelibrary.wiley.com/doi/10.1002/adma.202000953 by Pennsylvania State University, Wiley Online Library on [19/07/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

2000953 () www.advmat.de www.advancedsciencenews.com , 32, 2000953in the trained material group, which is in accordance with the similar crystal structure of these materials. For a more quantitative analysis, we selected two different predictors that are associated with the bandgaps and the crystal structures of the material, and the projected mean values and their standard deviations of each material based on these two predictors and the 16 2DMOINets are plotted into a histogram with an error bar as summarized in Figure 4a,b, respectively. Clear correlations between the projected values and the true physical parameters (bandgap in Figure 4a, and crystal structure in Figure 4b) are found. We can thus use the projected values as an indication of the probability of the physical property of interest of an unknown material belonging to each class (represented by each base vector of the corresponding predictor). Note that there are also misclassified instances, which we believe can be improved by either constructing a more complex material property predictor, or expanding the training data set in terms of both the number of images and the number of materials. This method can also be used for systematic studies of other factors such as the effect of different mechanical exfoliation techniques, bulk crystal qualities, and so on. Finally, we show that the trained 2DMOINet can be adapted for different applications through transfer learning. The basic idea is to use the trained 2DMOINet as the initialization for the new training problem rather than a random initialization. With this approach, we are able to train the 2DMOINet for new optical identification/characterization problems with a good bal-ance between prediction accuracy and computational/data cost. Here we use OM images of graphene synthesized by chemical vapor deposition (CVD) and exfoliated Td-WTe 2 as two examples. Figure 5a and Figure S53, Supporting Information, and Figure 5c and Figure S54, Supporting Information, display the test images, the ground truth label maps, as well as the corresponding prediction results after the trainings on the CVD graphene and Figure 5. Transfer learning for CVD graphene and exfoliated Td-WTe 2. a,c) Typical training results on the CVD graphene dataset (a) and on the exfoliated Td-WTe 2 dataset. The left column are OM images, the middle column are the ground truth label maps, and the right column are the label maps predicted by the re-trained 2DMOINet. Scale bars: 20 µm. b) Global test accuracy as a function of the number of images (CVD graphene) in the training dataset for the pre-training method (red) and the random initialization method (blue). d) Pixel-level confusion matrix on the test dataset of the 2DMOINet trained for exfoliated Td-WTe 2. The mean class accuracy reaches 91%. 15214095, 2020, 29, Downloaded from https://advanced.onlinelibrary.wiley.com/doi/10.1002/adma.202000953 by Pennsylvania State University, Wiley Online Library on [19/07/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

2000953 () www.advmat.de www.advancedsciencenews.com , 32, 2000953the exfoliated Td-WTe 2 datasets, respectively, with the transfer learning approach. As we can see, the prediction results match the ground truths very well. To compare the pre-training transfer learning approach with the conventional approach with random initialization, we plot the global test accuracy of networks trained with both approaches as a function of the number of OM images in the CVD-graphene dataset as shown in Figure 5b. Representative test images and their predictions can be found in Figure S53, Supporting Information. With the pre-training approach, we were able to achieve 67% global accuracy with only 60 training images, whereas at least 240 images are required for the conven-tional random initialization approach to reach comparable accuracy. In addition, we also showed in Figure 5d the pixel-level confusion matrix of the transfer-learning 2DMOINet on the Td-WTe 2 dataset. Because this newly trained 2DMOINet is dedicated to the thickness prediction for only one material, it is able to differentiate different thicknesses with much better accuracy (91% mean class accuracy) and finer thickness divisions. In summary, we develop a deep learning algorithm for the optical characterization of 2D materials and the extraction of deep graphical features from optical microscopy images that can be used for anticipating material properties. After training, the neural network can achieve rapid characterization of material identities and their thicknesses with high accuracy. A fully automated system utilizing this algorithm is developed, which is able to free up tremendous amount of time for researchers. In addition, a systematic analysis was made to understand how the network captures deep graphical features such as color, contrast, edges, shapes, and flakes sizes from the optical images. Based on this, an assemble approach derived from the trained neural network can be used for the prediction of physical properties of the materials. We also demonstrate that the trained network can be adapted for different optical char acterization applications with minimal additional training through a transfer learning technique. The proposed methodology can potentially be extended for identification and under standing other morphological or spectroscopic data of diverse nanomaterials. Supporting Information Supporting Information is available from the Wiley Online Library or from the author. Acknowledgements This material was based upon work sponsored in part by the U.S. Army Research Office through the Institute for Soldier Nanotechnologies, under cooperative agreement number W911NF-18-2-0048, AFOSR FATE MURI, Grant no. FA9550-15-1-0514, National Natural Science Foundation of China (Grant no. 41871240), the National Science Foundation Grant 2DARE (EFRI-1542815), NSF DMR-1507806, and the STC Center for Integrated Quantum Materials, NSF Grant no. DMR-599 1231319. Work by P.J.-H. and group was primarily funded by the DOE Office of Science, BES, under award DE-SC0019300 as well as the Gordon and Betty Moore Foundation via Grant GBMF4541. B.H. gratefully acknowledges the financial support from China Scholarship Council. H.K. and X.L. acknowledge the support from NSF under Grand no. 1945364.Conflict of Interest The authors declare no conflict of interest. Author Contributions B.H. and Y .L. contributed equally to this work. Y .L., Y .Y ., B.H., J.Y ., J.K., and T.P. conceived the experiment. B.H., Y .L., Y .Y ., and W.L. performed the training and testing of the 2DMOINet. B.H., Y .Y ., N.M., Y .L., and H.W. carried on the data labeling. B.H. and Y .L. did further analysis of the trained network. Y .Y ., K.Y ., and X.W. assisted with the setup of the automatic 2D-material-searching system. N.M., Y .Y ., Y .L., H.W., L.Z., J.I.-J.W., Q.M., J.L., Y .C., D.R.-L., Y .B, E.N.-M., D.K., D.M., V.F., S.W., H.K., and X.L. provided the microscopy images. J.K., J.Y ., P.J.-H., and T.P supervised this work. Y .L., B.H., Y .Y ., P.J.-H., J.K., J.Y ., and T.P. co-wrote the paper with inputs from all the authors. Keywords 2D materials, deep learning, machine learning, material characterization, optical microscopy Received: February 10, 2020 Revised: May 13, 2020 Published online: June 9, 2020 [1] A. K. Geim, K. S. Novoselov, Nat. Mater. 2007, 6, 183. [2] A. C. Ferrari, F. Bonaccorso, V. Fal’ko, K. S. Novoselov, S. Roche, P. Bøggild, S. Borini, F. H. L. Koppens, V. Palermo, N. Pugno, J. A. Garrido, R. Sordan, A. Bianco, L. Ballerini, M. Prato, E. Lidorikis, J. Kivioja, C. Marinelli, T. Ryhänen, A. Morpurgo, J. N. Coleman, V. Nicolosi, L. Colombo, A. Fert, M. Garcia-Hernandez, A. Bachtold, G. F. Schneider, F. Guinea, C. Dekker, M. Barbone, Z. Sun, C. Galiotis, A. N. Grigorenko, G. Konstantatos, A. Kis, M. Katsnelson, L. Vandersypen, A. Loiseau, V. Morandi, D. Neumaier, E. Treossi, V. Pellegrini, M. Polini, A. Tredicucci, G. M. Williams, B. H. Hong, J.-H. Ahn, J. M. Kim, H. Zirath, B. J. van Wees, H. van der Zant, L. Occhipinti, A. Di Matteo, I. A. Kinloch, T. Seyller, E. Quesnel, X. Feng, K. Teo, N. Rupesinghe, P. Hakonen, S. R. T. Neil, Q. Tannock, T. Löfwander, J. Kinaret, Nanoscale 2015, 7, 4598. [3] a) V. Nicolosi, M. Chhowalla, M. G. Kanatzidis, M. S. Strano, J. N. Coleman, Science 2013, 340, 1226419; b) Q. H. Wang, K. Kalantar-Zadeh, A. Kis, J. N. Coleman, M. S. Strano, Nat. Nanotechnol. 2012, 7, 699; c) C. Tan, X. Cao, X.-J. Wu, Q. He, J. Yang, X. Zhang, J. Chen, W. Zhao, S. Han, G.-H. Nam, M. Sindoro, H. Zhang, Chem. Rev. 2017, 117, 6225. [4] a) K. S. Novoselov, A. K. Geim, S. V. Morozov, D. Jiang, Y . Zhang, S. V. Dubonos, I. V. Grigorieva, A. A. Firsov, Science 2004, 306, 666; b) M. Yi, Z. Shen, J. Mater. Chem. A 2015, 3, 11700. [5] S. Masubuchi, M. Morimoto, S. Morikawa, M. Onodera, Y . Asakawa, K. Watanabe, T. Taniguchi, T. Machida, Nat. Commun. 2018, 9, 1413. [6] a) H. Li, J. Wu, X. Huang, G. Lu, J. Yang, X. Lu, Q. Xiong, H. Zhang, ACS Nano 2013, 7, 10344; b) X. Lin, Z. Si, W. Fu, J. Yang, S. Guo, Y . Cao, J. Zhang, X. Wang, P. Liu, K. Jiang, W. Zhao, Nano Res. 2018, 11, 6316; c) S. Masubuchi, T. Machida, npj 2D Mater. Appl. 2019, 3, 4; d) Z. H. Ni, H. M. Wang, J. Kasim, H. M. Fan, T. Yu, Y . H. Wu, Y . P. Feng, Z. X. Shen, Nano Lett. 2007, 7, 2758; e) C. M. Nolen, G. Denina, D. Teweldebrhan, B. Bhanu, A. A. Balandin, ACS Nano 2011, 5, 914; f) P. Blake, E. W. Hill, A. H. Castro Neto, 15214095, 2020, 29, Downloaded from https://advanced.onlinelibrary.wiley.com/doi/10.1002/adma.202000953 by Pennsylvania State University, Wiley Online Library on [19/07/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

2000953 () www.advmat.de www.advancedsciencenews.com , 32, 2000953K. S. Novoselov, D. Jiang, R. Yang, T. J. Booth, A. K. Geim, Appl. Phys. Lett. 2007, 91, 063124. [7] a) J. Long, E. Shelhamer, T. Darrell, in IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), Piscataway, NJ, USA 2015, pp. 3431–3440; b) L. Chen, G. Papandreou, I. Kokkinos, K. Murphy, A. L. Yuille, IEEE Trans. Pattern Anal. Mach. Intell. 2018, 40, 834; c) O. Ronneberger, P. Fischer, T. Brox, in Medical Image Computing and Computer-Assisted Intervention – MICCAI 2015, Lecture Notes in Computer Science, Vol. 9351, Springer, Cham, Switzerland 2015. [8] V. Badrinarayanan, A. Kendall, R. Cipolla, IEEE Trans. Pattern Anal. Mach. Intell. 2017, 39, 2481.[9] a) 2D Semiconductors, https://www.2dsemiconductors.com/ (accessed: January 2019); b) GQ Graphene, https://www.hqgraphene.com/ (accessed: January 2019). [10] I. Sutskever, J. Martens, G. Dahl, G. Hinton, in Proc. 30th Int. Conf. on Machine Learning, Vol. 28, Association for Computing Machinery, New York 2013. [11] K. Simonyan, A. Zisserman, arXiv:1409.1556, 2014. [12] C. Szegedy, L. Wei, J. Yangqing, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, A. Rabinovich, in 2015 IEEE Conf. on Com-puter Vision and Pattern Recognition (CVPR), Boston, MA, USA 2015, pp. 1–9. 15214095, 2020, 29, Downloaded from https://advanced.onlinelibrary.wiley.com/doi/10.1002/adma.202000953 by Pennsylvania State University, Wiley Online Library on [19/07/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

